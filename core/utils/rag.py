from core.settings import (
    openai_key,
    model_api_url,
    search_api_key,
    course_api_key
    )

from core.utils.memory_store import user_memories, get_user_memory
from core.utils.agents import search_hh, tools, agent_chain
import os

#from langchain_openai import ChatOpenAI
#from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferWindowMemory
from langchain.agents import initialize_agent, Tool, AgentType

from langchain.utilities import SerpAPIWrapper

search_tool = SerpAPIWrapper(serpapi_api_key=search_api_key)

# openai_key = openai_key
# model_name="gpt-4o-mini"
# llm = ChatOpenAI(openai_api_key=openai_key, base_url=model_api_url, model_name=model_name, temperature=1.2)

from core.utils.utils import ChatOpenAI
llm_stepik = ChatOpenAI(temperature=0.0, course_api_key=course_api_key)

# ##########################################
# ##########################################
# def load_all_texts_from_folder(folder_path):
#     """
#     –ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ *.txt –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
#     –∏ –∑–∞–º–µ–Ω—è–µ—Ç –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–±—ã—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.

#     :param folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã.
#     :return: –°—Ç—Ä–æ–∫–∞ —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤.
#     """
#     doc = ""
#     for file_name in os.listdir(folder_path):
#         # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–º
#         if file_name.endswith(".txt"):
#             file_path = os.path.join(folder_path, file_name)
#             with open(file_path, encoding="utf-8") as f:
#                 text = f.read()
#                 # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–±—ã—á–Ω—ã–µ
#                 text = text.replace("\xa0", " ")
#                 # –£–¥–∞–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã '#' –∏ '*'
#                 text = text.replace("#", "").replace("*", "")
#                 doc += text + "\n"  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
#     return doc

# # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
# folder_path = "./datasets/txt"
# doc = load_all_texts_from_folder(folder_path)
# ##########################################

# ######### SPLITTER #######################
# ##########################################
# splitter = RecursiveCharacterTextSplitter(
# chunk_size=1000,
# chunk_overlap=150,
# length_function=len,
# )
# split_documents = splitter.create_documents([doc])
# bm25 = BM25Retriever.from_documents(split_documents)  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ–º—É –Ω–µ –Ω—É–∂–Ω—ã
# bm25.k = 5  # –¢–∞–∫ –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

# ##########################################

# # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
# user_memories = {}

# # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è/—Å–æ–∑–¥–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# def get_user_memory(user_id):
#     if user_id not in user_memories:
#         # —Å–æ–∑–¥–∞–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
#         user_memories[user_id] = ConversationBufferWindowMemory(
#             memory_key='history',
#             k=6,
#             input_key='question'
#         )
#     return user_memories[user_id]


# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
def chat_bot(user_id: int, query: str) -> str:

    # –ø–æ–ª—É—á–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_memory = get_user_memory(user_id)
    print(user_memory)

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    # –µ—Å–ª–∏ —É –≤–∞—Å –Ω–µ—Ç –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã, —É–∫–∞–∂–∏—Ç–µ 'device': 'cpu'
    hf_embeddings_model = HuggingFaceEmbeddings(
        model_name="cointegrated/LaBSE-en-ru", model_kwargs={"device": "cpu"}
    )

    # –ø–æ–¥–∫–ª—é—á–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    db = FAISS.load_local(
        "./core/db/faiss_db", hf_embeddings_model, allow_dangerous_deserialization=True
        )

    # ### –û–ø—Ä–µ–¥–µ–ª–∏–º üé£ Retriever
    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä
    retriever = db.as_retriever(
        search_type="similarity",  # —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        k=30,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Default: 4)
        score_threshold=None,  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–∏—Å–∫–∞ "similarity_score_threshold"
    )

    # ensemble_retriever = EnsembleRetriever(
    #         retrievers=[bm25, retriever],  # —Å–ø–∏—Å–æ–∫ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤
    #         weights=[
    #             0.4,
    #             0.6,
    #         ],  # –≤–µ—Å–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–º–Ω–æ–∂–∞–µ—Ç—Å—è —Å–∫–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç –∫–∞–∂–¥–æ–≥–æ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    # )

    # template = """You're a career counseling specialist. Your job is to answer candidates' questions and advise on the career track.
    # Answer concisely and in the same language. Separate text into paragraphs, if necessary.
    # Answer the question based only on the following context.
    # If the context doesn't answer the question, answer verbatim '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.'

    # {history}

    # {context}

    # Question: {question}
    # """

    # –ø—Ä–æ—Å—Ç–æ–π —à–∞–±–ª–æ–Ω —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏
    template = """You're a career counseling specialist. Your job is to answer candidates' questions and advise on the career track only.
    Answer concisely and in the same language. Separate text into paragraphs, if necessary.
    Don't use the pretrained data you were trained on to answer.
    If you don't know the answer to a question, answer '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.'.
    Answer the question based only on the following context:

    {history}

    {context}

    Question: {question}
    """
    # —Å–æ–∑–¥–∞—ë–º –ø—Ä–æ–º–ø—Ç –∏–∑ —à–∞–±–ª–æ–Ω–∞
    prompt = PromptTemplate.from_template(template)


    # –æ–±—ä—è–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å —Å—Ç—Ä–æ–∫—É –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    # —Å–æ–∑–¥–∞—ë–º —Ü–µ–ø–æ—á–∫—É
    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
            "history": lambda _: user_memory,
        }
        | prompt
        | llm_stepik
        | StrOutputParser()
    )

    # –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
    answer = chain.invoke(query)

    # –ï—Å–ª–∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –¥–∞—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥–µ–Ω—Ç–∞
    if "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É." in answer:
        try:
            # –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∏—Ç, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç HH Search
            agent_answer = agent_chain.run(input=query, memory=user_memory)
            if agent_answer:
                answer = agent_answer
            else:
                raise ValueError("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç HH Search –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
        except Exception as e:
            print(f'–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞: {e}')
            answer = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."

    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞–º—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_memory.save_context({"question": query}, {"output": answer})

    # if "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É." in answer:
    #     try:
    #         # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é SerpAPI
    #         serp_context = search_tool.run(f'{query} site:hh.ru')
    #         print(serp_context)

    #         if not serp_context:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π
    #             raise ValueError("SerpAPI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")

    #         serp_chain = (
    #             {
    #                 "context": lambda _: serp_context,
    #                 "question": RunnablePassthrough(),
    #                 "history": lambda _: user_memory,
    #             }
    #             | prompt
    #             | llm_stepik
    #             | StrOutputParser()
    #         )
    #         answer = serp_chain.invoke(query)

    #     except Exception as e:
    #         print(f'–û—à–∏–±–∫–∞ serp: {e}')
    #         return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."

    # –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞–º—è—Ç—å
    user_memory.save_context({"question": query}, {"output": answer})

    return answer